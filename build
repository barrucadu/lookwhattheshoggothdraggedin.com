#!/usr/bin/env python3

"""build

Usage:
  build [--drafts] [--out=<dir>] [--root=<url>]
  build (-h | --help)

Options:
  -h --help       Show this screen.
  --drafts        Include draft posts and pages.
  --out=<dir>     Directory to generate site in [default: _site]
  --root=<url>    Root of the website [default: https://www.lookwhattheshoggothdraggedin.com/]
"""

import hashlib
import jinja2
import json
import markdown
import os
import sys
import yaml

from distutils import dir_util, file_util
from docopt import docopt

import markdown_extensions

args = docopt(__doc__)
OUT_DIR = args["--out"]
BASE_HREF = args["--root"]
DRAFT_MODE = args["--drafts"]

JINJA2_ENV = jinja2.Environment(
    autoescape=jinja2.select_autoescape(["html", "xml"]),
    loader=jinja2.FileSystemLoader("_templates"),
)

with open("config/topics.yaml") as f:
    TOPICS = yaml.safe_load(f)

with open("config/systems.yaml") as f:
    SYSTEMS = yaml.safe_load(f)

with open("config/glossary.yaml") as f:
    GLOSSARY = yaml.safe_load(f)


EXTENSIONS = markdown_extensions.setup(GLOSSARY)


def do_markdown(text):
    return markdown.markdown(
        text,
        output_format="html5",
        extensions=[
            "admonition",
            "smarty",
            "tables",
            *[extension() for extension in EXTENSIONS],
        ],
    )


def read_files_from(fdir, permalink_base_href=BASE_HREF, include_drafts=False):
    out = []
    for fname in os.listdir(fdir):
        fpath = os.path.join(fdir, fname)
        if not os.path.isfile(fpath):
            continue

        with open(fpath, "r") as f:
            lines = f.readlines()
            metadata_end_idx = lines[1:].index("---\n") + 1
            text_start_idx = metadata_end_idx + 1
            metadata = yaml.load("".join(lines[1:metadata_end_idx]), Loader=yaml.SafeLoader)
            if metadata.get("draft", False) and not include_drafts:
                continue
            metadata["text"] = "".join(lines[text_start_idx:])
            metadata["body"] = do_markdown(metadata["text"])
            if "slug" not in metadata:
                metadata["slug"], _ = os.path.splitext(fname)
            metadata["permalink"] = f"{permalink_base_href}{metadata['slug']}.html"
            out.append(metadata)
    return out


ALL_PAGES = read_files_from("pages", include_drafts=DRAFT_MODE)

ALL_POSTS = sorted(
    read_files_from("posts", permalink_base_href=f"{BASE_HREF}post/", include_drafts=DRAFT_MODE),
    key=lambda post: post["date"],
    reverse=True,
)

ALL_POSTS_EXCLUDING_UNLISTED = [post for post in ALL_POSTS if not post.get("unlisted", False)]

for key, defn in GLOSSARY.items():
    GLOSSARY[key]["body"] = do_markdown(defn["text"])

for post in ALL_POSTS:
    if post.get("topic") not in TOPICS:
        print(f"{post['slug']} has invalid topic.")
        sys.exit(1)
    if post.get("system") is not None and post["system"] not in SYSTEMS:
        print(f"{post['slug']} has invalid system.")
        sys.exit(1)

    post["year"] = post["date"].strftime("%Y")
    post["month"] = post["date"].strftime("%b")
    post["day"] = post["date"].strftime("%-d")
    post["atom_date"] = post["date"].strftime("%Y-%m-%dT%H:%M:%S%z")
    post["excerpt"] = do_markdown(post["excerpt"])

YEARS = {post["year"]: post["year"] for post in ALL_POSTS_EXCLUDING_UNLISTED}

dir_util.copy_tree("static", OUT_DIR)
dir_util.mkpath(os.path.join(OUT_DIR, "post"))
dir_util.mkpath(os.path.join(OUT_DIR, "system"))
dir_util.mkpath(os.path.join(OUT_DIR, "topic"))
dir_util.mkpath(os.path.join(OUT_DIR, "year"))

HASHED_LINKS = {}
for fname in os.listdir("hashed-static"):
    fpath = os.path.join("hashed-static", fname)
    if not os.path.isfile(fpath):
        continue

    file_hash = hashlib.sha256()
    with open(fpath, "rb") as f:
        while True:
            data = f.read(65536)
            if not data:
                break
            file_hash.update(data)
    base, ext = os.path.splitext(fname)
    HASHED_LINKS[fname] = f"{base}-sha256-{file_hash.hexdigest()}{ext}"
    file_util.copy_file(fpath, os.path.join(OUT_DIR, HASHED_LINKS[fname]))


def find_file_by_slug(directory, slug, default=None):
    for ext in ["png", "jpg"]:
        fname = f"{slug}.{ext}"
        if os.path.isfile(os.path.join("static", directory, fname)):
            return f"{directory}/{fname}"
    return default


def render(link, template, **metadata):
    slug = link.split("/")[-1].split(".")[0]
    twitter_card = find_file_by_slug("twitter-cards", slug, default="twitter-card.jpg")

    with open(os.path.join(OUT_DIR, link), "w") as f:
        rendered = JINJA2_ENV.get_template(template).render(
            base_href=BASE_HREF,
            topics=TOPICS,
            systems=SYSTEMS,
            years=YEARS,
            draft_mode=DRAFT_MODE,
            hashed_links=HASHED_LINKS,
            permalink=f"{BASE_HREF}{link}",
            link=link,
            twitter_card=twitter_card,
            **metadata,
        )
        print(rendered, file=f)


def render_json_feed(link, title, posts):
    with open(os.path.join(OUT_DIR, link), "w") as f:
        feed = {
            "version": "https://jsonfeed.org/version/1.1",
            "title": title,
            "home_page_url": BASE_HREF,
            "feed_url": f"{BASE_HREF}{link}",
            "authors": [
                {
                    "name": "Michael Walker",
                    "email": "mike@barrucadu.co.uk",
                },
            ],
            "items": [
                {
                    "id": post["permalink"],
                    "url": post["permalink"],
                    "content_text": post["excerpt"],
                    "date_published": post["date"].strftime("%Y-%m-%dT%H:%M:%S%z"),
                }
                for post in posts
            ],
        }

        json.dump(feed, f)


def render_feeds_for(link_without_format, title, posts):
    render(
        link_without_format + ".xml",
        "atom.xml",
        title=title,
        posts=posts,
    )
    render_json_feed(
        link_without_format + ".json",
        title=title,
        posts=posts,
    )


render(
    "index.html",
    "archive.html",
    title="Articles",
    posts=ALL_POSTS_EXCLUDING_UNLISTED,
)

render(
    "glossary.html",
    "glossary.html",
    title="Glossary",
    glossary=GLOSSARY,
)

render_feeds_for("feed", "Recent Posts", ALL_POSTS_EXCLUDING_UNLISTED)

for field, entries in [("topic", TOPICS), ("system", SYSTEMS), ("year", YEARS)]:
    for slug, name in entries.items():
        matching_posts = [post for post in ALL_POSTS_EXCLUDING_UNLISTED if post.get(field) == slug]

        render(
            f"{field}/{slug}.html",
            "archive.html",
            title=name,
            posts=matching_posts,
        )

        render_feeds_for(f"{field}/{slug}", name, matching_posts)

for post in ALL_POSTS:
    render(f"post/{post['slug']}.html", "post.html", title=post["title"], post=post)

for page in ALL_PAGES:
    render(
        f"{page['slug']}.html",
        "page.html",
        title=page["title"],
        page=page,
    )
